{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["rmhej-N6vlEe"],"authorship_tag":"ABX9TyNd6bGtUUZYJ0KjhDdB5vyr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["전체 구조 : Affine-ReLU => Conv-ReLU-(Pooling)"],"metadata":{"id":"UXfTpHKyuCdN"}},{"cell_type":"markdown","source":["## 7.2 합성곱 계층\n","* fully connected의 문제점 : 데이터의 형상이 무시된다\n","* (책에서의 정의) **feature map** : 입출력 데이터\n","* **fused multiply-add (FMA)** : feature map과 filter에서 대응하는 원소끼리 곱한 후 그 총합을 구하는 것\n","  * 이 연산 때문에 3채널에 대응하는 필터 3개를 사용하더라도 output은 1개만 나오는 것이다\n","* padding은 주로 output size를 조정하려는 목적으로 사용한다   \n","  e.g. (4,4)에 (3,3) 필터를 패딩없이 적용하면 출력 사이즈가 2 줄어듦 -> 합성곱 연산이 반복되는 DNN에서는 문제가 될 수 있다 (어느 시점엔 출력 사이즈가 1이 되어 합성곱 더 못함)\n","$$OH=\\frac{H+2P-FH}{S}+1 \\\\ OW=\\frac{W+2P-FW}{S}+1$$   \n","where input size is (H, W), filter size is (FH, FW)\n","* 이미지 데이터는 3채널이니까 filter 3개로, 하지만 output이 3개가 되진 않는다\n","  * 하나의 input 덩어리의 각 채널에 대응하는 필터들은 서로 같은 크기여야 함\n","  * 그렇다면 output은 항상 1개만 내보낼 수 있는가: 아니다, 필터를 다수 사용할 수 있다: (C,H,W)인 한덩이 input에 FN개의 필터(FN,C,H,W)를 적용하면 output map도 FN개가 생긴다(FN, OH, OW)\n","  * Bias는 (FN, 1, 1)이 되겠다\n","\n","배치처리\n","* (N, C, H, W) -> (N, FN, OH, OW)\n","\n","\n","\n","\n","\n"],"metadata":{"id":"p2XAaC08vxhB"}},{"cell_type":"markdown","source":["## 7.3 풀링 계층\n","* e.g. max pooling, average pooling\n","  * 이미지 인식 분야에서는 주로 최대 풀링 사용\n","* pooling 할 때는 window size와 stride를 같게 하는게 보통\n"],"metadata":{"id":"VhLXbvV3SIrN"}},{"cell_type":"markdown","source":["## 7.4 합성곱/풀링 계층 구현하기\n","* Numpy에서는 원소에 접근할 때 for문을 안쓰는게 바람직하다\n"],"metadata":{"id":"L5YY_PIsVDGd"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"3_H67I4hRnPF","executionInfo":{"status":"ok","timestamp":1727185352816,"user_tz":-540,"elapsed":2,"user":{"displayName":"최승훈","userId":"08625075445342019551"}}},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","source":["x = np.random.rand(10, 1, 28, 28)\n","x.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I8s8zmqnWhwG","executionInfo":{"status":"ok","timestamp":1727002224031,"user_tz":-540,"elapsed":325,"user":{"displayName":"최승훈","userId":"08625075445342019551"}},"outputId":"482b652a-0528-4307-de49-ed9bd15f645d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10, 1, 28, 28)"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["x[0, 0].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cJWETQsSWqJV","executionInfo":{"status":"ok","timestamp":1727002246379,"user_tz":-540,"elapsed":316,"user":{"displayName":"최승훈","userId":"08625075445342019551"}},"outputId":"3bf170e0-1b40-45e7-c052-4292cc74c92f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(28, 28)"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["### 7.4.2 im2col로 데이터 전개하기\n","* Numpy에서는 원소에 접근할 때 for문을 안쓰는게 바람직하다\n","* im2col은 입력 데이터를 필터링하기 좋게 전개하는 함수\n","  * 4차원 입력 데이터에서 하나의 필터가 적용되는 박스를 1줄로\n","* 보기에 좋게끔 stride를 크게 잡아 필터의 적용 영역이 겹치지 않도록 했지만, 실제 상황에서는 영역이 겹치는 경우가 대부분이다   \n","  -> im2col로 전개 후 원소 수가 원래 블록의 원소 수보다 많아진다   \n","  (im2col은 메모리 비효율적인 방식)   \n","  (컴퓨터는 큰 행렬을 묶어서 계산하는데 탁월하다 - 선형 대수 라이브러리로 큰 행렬의 곱셉을 빠르게!)\n","* 이제 입력데이터가 행렬이 됐으니, 필터 역시 행렬형태로 만들어서 곱\n"],"metadata":{"id":"qDPI2YMUfQHQ"}},{"cell_type":"code","source":["def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n","    \"\"\"다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화).\n","\n","    Parameters\n","    ----------\n","    input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비)\n","    filter_h : 필터의 높이\n","    filter_w : 필터의 너비\n","    stride : 스트라이드\n","    pad : 패딩\n","\n","    Returns\n","    -------\n","    col : 2차원 배열\n","    \"\"\"\n","    N, C, H, W = input_data.shape\n","    out_h = (H + 2*pad - filter_h)//stride + 1\n","    out_w = (W + 2*pad - filter_w)//stride + 1\n","\n","    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n","    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n","\n","    for y in range(filter_h):\n","        y_max = y + stride*out_h\n","        for x in range(filter_w):\n","            x_max = x + stride*out_w\n","            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n","\n","    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n","    return col\n"],"metadata":{"id":"fdiLYpx_WsBM","executionInfo":{"status":"ok","timestamp":1727185434789,"user_tz":-540,"elapsed":389,"user":{"displayName":"최승훈","userId":"08625075445342019551"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["### 개인 테스트 - numpy의 transpose, reshape"],"metadata":{"id":"rmhej-N6vlEe"}},{"cell_type":"code","source":["def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n","    \"\"\"다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화).\n","\n","    Parameters\n","    ----------\n","    input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비)\n","    filter_h : 필터의 높이\n","    filter_w : 필터의 너비\n","    stride : 스트라이드\n","    pad : 패딩\n","\n","    Returns\n","    -------\n","    col : 2차원 배열\n","    \"\"\"\n","    N, C, H, W = input_data.shape\n","    out_h = (H + 2*pad - filter_h)//stride + 1\n","    out_w = (W + 2*pad - filter_w)//stride + 1\n","\n","    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n","    print(\"img:\\n\",img)\n","    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n","    print(\"col:\\n\",col)\n","    for y in range(filter_h):\n","        y_max = y + stride*out_h\n","        for x in range(filter_w):\n","            x_max = x + stride*out_w\n","            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n","    print(\"after:\\n\", col)\n","    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n","    return col\n","\n","\n","x = np.array([[[[1,2,3],[4,5,6],[7,8,9]],[[-1,-2,-3],[-4,-5,-6],[-7,-8,-9]]],[[[11,12,13],[14,15,16],[17,18,19]],[[-11,-12,-13],[-14,-15,-16],[-17,-18,-19]]]])\n","a = im2col(x, 2, 2, stride=1, pad=0)\n","print(a)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WERHale-dLZl","executionInfo":{"status":"ok","timestamp":1727093553225,"user_tz":-540,"elapsed":413,"user":{"displayName":"최승훈","userId":"08625075445342019551"}},"outputId":"7ff7d631-33c8-4869-9032-9723a652c251"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["img:\n"," [[[[  1   2   3]\n","   [  4   5   6]\n","   [  7   8   9]]\n","\n","  [[ -1  -2  -3]\n","   [ -4  -5  -6]\n","   [ -7  -8  -9]]]\n","\n","\n"," [[[ 11  12  13]\n","   [ 14  15  16]\n","   [ 17  18  19]]\n","\n","  [[-11 -12 -13]\n","   [-14 -15 -16]\n","   [-17 -18 -19]]]]\n","col:\n"," [[[[[[0. 0.]\n","     [0. 0.]]\n","\n","    [[0. 0.]\n","     [0. 0.]]]\n","\n","\n","   [[[0. 0.]\n","     [0. 0.]]\n","\n","    [[0. 0.]\n","     [0. 0.]]]]\n","\n","\n","\n","  [[[[0. 0.]\n","     [0. 0.]]\n","\n","    [[0. 0.]\n","     [0. 0.]]]\n","\n","\n","   [[[0. 0.]\n","     [0. 0.]]\n","\n","    [[0. 0.]\n","     [0. 0.]]]]]\n","\n","\n","\n","\n"," [[[[[0. 0.]\n","     [0. 0.]]\n","\n","    [[0. 0.]\n","     [0. 0.]]]\n","\n","\n","   [[[0. 0.]\n","     [0. 0.]]\n","\n","    [[0. 0.]\n","     [0. 0.]]]]\n","\n","\n","\n","  [[[[0. 0.]\n","     [0. 0.]]\n","\n","    [[0. 0.]\n","     [0. 0.]]]\n","\n","\n","   [[[0. 0.]\n","     [0. 0.]]\n","\n","    [[0. 0.]\n","     [0. 0.]]]]]]\n","after:\n"," [[[[[[  1.   2.]\n","     [  4.   5.]]\n","\n","    [[  2.   3.]\n","     [  5.   6.]]]\n","\n","\n","   [[[  4.   5.]\n","     [  7.   8.]]\n","\n","    [[  5.   6.]\n","     [  8.   9.]]]]\n","\n","\n","\n","  [[[[ -1.  -2.]\n","     [ -4.  -5.]]\n","\n","    [[ -2.  -3.]\n","     [ -5.  -6.]]]\n","\n","\n","   [[[ -4.  -5.]\n","     [ -7.  -8.]]\n","\n","    [[ -5.  -6.]\n","     [ -8.  -9.]]]]]\n","\n","\n","\n","\n"," [[[[[ 11.  12.]\n","     [ 14.  15.]]\n","\n","    [[ 12.  13.]\n","     [ 15.  16.]]]\n","\n","\n","   [[[ 14.  15.]\n","     [ 17.  18.]]\n","\n","    [[ 15.  16.]\n","     [ 18.  19.]]]]\n","\n","\n","\n","  [[[[-11. -12.]\n","     [-14. -15.]]\n","\n","    [[-12. -13.]\n","     [-15. -16.]]]\n","\n","\n","   [[[-14. -15.]\n","     [-17. -18.]]\n","\n","    [[-15. -16.]\n","     [-18. -19.]]]]]]\n","[[  1.   2.   4.   5.  -1.  -2.  -4.  -5.]\n"," [  2.   3.   5.   6.  -2.  -3.  -5.  -6.]\n"," [  4.   5.   7.   8.  -4.  -5.  -7.  -8.]\n"," [  5.   6.   8.   9.  -5.  -6.  -8.  -9.]\n"," [ 11.  12.  14.  15. -11. -12. -14. -15.]\n"," [ 12.  13.  15.  16. -12. -13. -15. -16.]\n"," [ 14.  15.  17.  18. -14. -15. -17. -18.]\n"," [ 15.  16.  18.  19. -15. -16. -18. -19.]]\n"]}]},{"cell_type":"code","source":["x2=np.arange(48).reshape(4,12)\n","x2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dGpcZOOyuwdV","executionInfo":{"status":"ok","timestamp":1727075770202,"user_tz":-540,"elapsed":499,"user":{"displayName":"최승훈","userId":"08625075445342019551"}},"outputId":"bc2e840e-d76e-4fa5-b877-56abbd7ac028"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n","       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n","       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n","       [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["x2 = x2.reshape(4,2,6)\n","x2\n","# x2.reshape(4,6,2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"atoh0K57vP3v","executionInfo":{"status":"ok","timestamp":1727077116279,"user_tz":-540,"elapsed":499,"user":{"displayName":"최승훈","userId":"08625075445342019551"}},"outputId":"20026762-d8a0-4c7e-8a0e-dda9a871e6a2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[ 0,  1,  2,  3,  4,  5],\n","        [ 6,  7,  8,  9, 10, 11]],\n","\n","       [[12, 13, 14, 15, 16, 17],\n","        [18, 19, 20, 21, 22, 23]],\n","\n","       [[24, 25, 26, 27, 28, 29],\n","        [30, 31, 32, 33, 34, 35]],\n","\n","       [[36, 37, 38, 39, 40, 41],\n","        [42, 43, 44, 45, 46, 47]]])"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["x2_trans = x2.transpose(0,2,1)\n","x2_trans"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_z6g88lNvemV","executionInfo":{"status":"ok","timestamp":1727076167696,"user_tz":-540,"elapsed":459,"user":{"displayName":"최승훈","userId":"08625075445342019551"}},"outputId":"ba6e3c02-70f7-47af-d391-1fce5f71e6b9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[ 0,  6],\n","        [ 1,  7],\n","        [ 2,  8],\n","        [ 3,  9],\n","        [ 4, 10],\n","        [ 5, 11]],\n","\n","       [[12, 18],\n","        [13, 19],\n","        [14, 20],\n","        [15, 21],\n","        [16, 22],\n","        [17, 23]],\n","\n","       [[24, 30],\n","        [25, 31],\n","        [26, 32],\n","        [27, 33],\n","        [28, 34],\n","        [29, 35]],\n","\n","       [[36, 42],\n","        [37, 43],\n","        [38, 44],\n","        [39, 45],\n","        [40, 46],\n","        [41, 47]]])"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["x2_trans.reshape(24,-1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A8NdC5bmwk17","executionInfo":{"status":"ok","timestamp":1727076159164,"user_tz":-540,"elapsed":462,"user":{"displayName":"최승훈","userId":"08625075445342019551"}},"outputId":"6e731cb8-024c-479c-ca68-ddbcba7b3cad"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0,  6],\n","       [ 1,  7],\n","       [ 2,  8],\n","       [ 3,  9],\n","       [ 4, 10],\n","       [ 5, 11],\n","       [12, 18],\n","       [13, 19],\n","       [14, 20],\n","       [15, 21],\n","       [16, 22],\n","       [17, 23],\n","       [24, 30],\n","       [25, 31],\n","       [26, 32],\n","       [27, 33],\n","       [28, 34],\n","       [29, 35],\n","       [36, 42],\n","       [37, 43],\n","       [38, 44],\n","       [39, 45],\n","       [40, 46],\n","       [41, 47]])"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["### 구현"],"metadata":{"id":"OKHtwT2owPff"}},{"cell_type":"code","source":["x1 = np.random.rand(1, 3, 7, 7)\n","col1 = im2col(x1, 5, 5, stride=1, pad=0)\n","print(col1.shape)\n","\n","x2 = np.random.rand(10, 3, 7, 7)\n","col2 = im2col(x2, 5, 5, stride=1, pad=0)\n","print(col2.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nk1YxyNoXCv_","executionInfo":{"status":"ok","timestamp":1727091756426,"user_tz":-540,"elapsed":394,"user":{"displayName":"최승훈","userId":"08625075445342019551"}},"outputId":"837a717d-1d28-422b-cd20-d889056d08ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(9, 75)\n","(90, 75)\n"]}]},{"cell_type":"code","source":["class Convolution:\n","  def __init__(self, W, b, stride=1, pad=0):\n","    self.W = W\n","    self.b = b\n","    self.stride = stride\n","    self.pad = pad\n","\n","  def forward(self, x):\n","    N, C, H, W = x.shape\n","    FN, C, FH, FW = self.W.shape\n","    out_H = 1 + (H + 2*self.pad - FH) / self.stride\n","    out_W = 1 + (W + 2*self.pad - FW) / self.stride\n","\n","    col = im2col(x, FH, FW, self.stride, self.pad)\n","    col_W = self.W.reshape(FN, -1).T\n","    out = np.dot(col, col_W) + self.b\n","    out = out.reshape(N, out_H, out_W, -1).transpose(0, 3, 1, 2)\n","    return out\n"],"metadata":{"id":"ogjTs71Yk1gF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n","    \"\"\"(im2col과 반대) 2차원 배열을 입력받아 다수의 이미지 묶음으로 변환한다.\n","\n","    Parameters\n","    ----------\n","    col : 2차원 배열(입력 데이터)\n","    input_shape : 원래 이미지 데이터의 형상（예：(10, 1, 28, 28)）\n","    filter_h : 필터의 높이\n","    filter_w : 필터의 너비\n","    stride : 스트라이드\n","    pad : 패딩\n","\n","    Returns\n","    -------\n","    img : 변환된 이미지들\n","    \"\"\"\n","    N, C, H, W = input_shape\n","    out_h = (H + 2*pad - filter_h)//stride + 1\n","    out_w = (W + 2*pad - filter_w)//stride + 1\n","    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n","\n","    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n","    for y in range(filter_h):\n","        y_max = y + stride*out_h\n","        for x in range(filter_w):\n","            x_max = x + stride*out_w\n","            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n","\n","    return img[:, :, pad:H + pad, pad:W + pad]"],"metadata":{"id":"voFNezwYoSh_","executionInfo":{"status":"ok","timestamp":1727193903460,"user_tz":-540,"elapsed":413,"user":{"displayName":"최승훈","userId":"08625075445342019551"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["class Convolution:\n","  def __init__(self, W, b, stride=1, pad=0):\n","    self.W = W\n","    self.b = b\n","    self.stride = stride\n","    self.pad = pad\n","\n","    self.x = None\n","    self.col = None\n","    self.col_W = None\n","\n","    self.dW = None\n","    self.db = None\n","\n","  def forward(self, x):\n","    FN, C, FH, FW = self.W.shape\n","    N, C, H, W = x.shape\n","    out_h = int(1 + (H + 2*self.pad - FH) / self.stride)\n","    out_w = int(1 + (W + 2*self.pad - FW) / self.stride)\n","\n","    col = im2col(x, FH, FW, self.stride, self.pad)\n","    col_W = self.W.reshape(FN, -1).T\n","    out = np.dot(col, col_W) + self.b\n","    out = out.reshape(N, out_h, out_w, -1).transpose(0,3,1,2)\n","\n","    self.x = x\n","    self.col = col\n","    self.col_W = col_W\n","\n","    return out\n","\n","  # backward 추가\n","  def backward(self, dout):\n","    FN, C, FH, FW = self.W.shape\n","    dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n","\n","    self.db = np.sum(dout, axis=0)\n","    self.dW = np.dot(self.col.T, dout)\n","    # out.reshape 하던거 참조\n","    self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n","\n","    dcol = np.dot(dout, self.col_W.T)\n","    dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n","\n","    return dx"],"metadata":{"id":"ADBYtvrSomtE","executionInfo":{"status":"ok","timestamp":1727195815616,"user_tz":-540,"elapsed":409,"user":{"displayName":"최승훈","userId":"08625075445342019551"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["class Pooling:\n","  def __init__(self, pool_h, pool_w, stride=1, pad=0):\n","    self.pool_h = pool_h\n","    self.pool_w = pool_w\n","    self.stride = stride\n","    self.pad = pad\n","\n","    self.x = None\n","    self.arg_max = None\n","\n","  def forward(self, x):\n","    N, C, H, W = x.shape\n","    out_h = int(1 + (H - self.pool_h) / self.stride)\n","    out_w = int(1 + (W - self.pool_w) / self.stride)\n","    print(out_h, out_w)\n","    col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n","    col = col.reshape(-1, self.pool_h*self.pool_w)\n","\n","    arg_max = np.argmax(col, axis=1)\n","    out = np.max(col, axis=1)\n","    out = out.reshape(N, out_h, out_w, C).transpose(0,3,1,2)\n","\n","    self.x = x\n","    self.arg_max = arg_max\n","\n","    return out\n","\n","  def backward(self, dout):\n","    # C-dim 다시 맨 뒤로 돌리고\n","    dout = dout.transpose(0,2,3,1)\n","\n","    pool_size = self.pool_h * self.pool_w\n","    dmax = np.zeros((dout.size, pool_size))\n","    dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n","\n","    dmax = dmax.reshape(dout.shape + (pool_size, ))\n","    # C-dim 뒤쪽 차원으로 병합\n","    dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n","    dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n","    return dx\n","\n","x1 = np.random.rand(3,3,4,4)\n","dout = np.random.rand(3,3,2,2)\n","layer = Pooling(2,2,2,0)\n","layer.forward(x1)\n","layer.arg_max.flatten()\n","layer.backward(dout)"],"metadata":{"id":"25yhr8SOpPIQ","executionInfo":{"status":"ok","timestamp":1727193906933,"user_tz":-540,"elapsed":508,"user":{"displayName":"최승훈","userId":"08625075445342019551"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3e846c03-ae8a-4f4e-8efc-3fc0d2ada521"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["2 2\n","(3, 2, 2, 3)\n","(3, 2, 2, 3, 4)\n"]},{"output_type":"execute_result","data":{"text/plain":["array([[[[0.        , 0.        , 0.        , 0.67044796],\n","         [0.        , 0.52244178, 0.        , 0.        ],\n","         [0.        , 0.05853519, 0.00914409, 0.        ],\n","         [0.        , 0.        , 0.        , 0.        ]],\n","\n","        [[0.21072556, 0.        , 0.23189859, 0.        ],\n","         [0.        , 0.        , 0.        , 0.        ],\n","         [0.28335701, 0.        , 0.40706139, 0.        ],\n","         [0.        , 0.        , 0.        , 0.        ]],\n","\n","        [[0.38333943, 0.        , 0.        , 0.        ],\n","         [0.        , 0.        , 0.05733793, 0.        ],\n","         [0.        , 0.20664383, 0.24172443, 0.        ],\n","         [0.        , 0.        , 0.        , 0.        ]]],\n","\n","\n","       [[[0.        , 0.        , 0.70992835, 0.        ],\n","         [0.        , 0.20110017, 0.        , 0.        ],\n","         [0.        , 0.        , 0.        , 0.        ],\n","         [0.        , 0.16316316, 0.15891281, 0.        ]],\n","\n","        [[0.        , 0.        , 0.        , 0.        ],\n","         [0.3701263 , 0.        , 0.48895925, 0.        ],\n","         [0.        , 0.        , 0.        , 0.        ],\n","         [0.        , 0.2704038 , 0.20481213, 0.        ]],\n","\n","        [[0.        , 0.78356124, 0.        , 0.        ],\n","         [0.        , 0.        , 0.9072011 , 0.        ],\n","         [0.        , 0.        , 0.04263905, 0.        ],\n","         [0.        , 0.13278212, 0.        , 0.        ]]],\n","\n","\n","       [[[0.        , 0.        , 0.        , 0.21314934],\n","         [0.        , 0.87723261, 0.        , 0.        ],\n","         [0.76046851, 0.        , 0.        , 0.        ],\n","         [0.        , 0.        , 0.40771194, 0.        ]],\n","\n","        [[0.        , 0.        , 0.        , 0.45643068],\n","         [0.        , 0.17966601, 0.        , 0.        ],\n","         [0.82084712, 0.        , 0.        , 0.        ],\n","         [0.        , 0.        , 0.47802282, 0.        ]],\n","\n","        [[0.        , 0.28134794, 0.39401121, 0.        ],\n","         [0.        , 0.        , 0.        , 0.        ],\n","         [0.12869667, 0.        , 0.        , 0.50386583],\n","         [0.        , 0.        , 0.        , 0.        ]]]])"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["## 7.5 CNN 구현하기"],"metadata":{"id":"-9wuxlM6pare"}},{"cell_type":"code","source":["from collections import OrderedDict\n","\n","def softmax(x):\n","    if x.ndim == 2:\n","        x = x.T\n","        x = x - np.max(x, axis=0)\n","        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n","        return y.T\n","\n","    x = x - np.max(x) # 오버플로 대책\n","    return np.exp(x) / np.sum(np.exp(x))\n","\n","def cross_entropy_error(y, t):\n","    if y.ndim == 1:\n","        t = t.reshape(1, t.size)\n","        y = y.reshape(1, y.size)\n","\n","    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n","    if t.size == y.size:\n","        t = t.argmax(axis=1)\n","\n","    batch_size = y.shape[0]\n","    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n","\n","\n","class Relu:\n","  def __init__(self):\n","      self.mask = None\n","\n","  def forward(self, x):\n","      self.mask = (x <= 0)\n","      out = x.copy()\n","      out[self.mask] = 0\n","\n","      return out\n","\n","  def backward(self, dout):\n","      dout[self.mask] = 0\n","      dx = dout\n","\n","      return dx\n","\n","class Affine:\n","  def __init__(self, W, b):\n","      self.W = W\n","      self.b = b\n","\n","      self.x = None\n","      self.original_x_shape = None\n","      # 가중치와 편향 매개변수의 미분\n","      self.dW = None\n","      self.db = None\n","\n","  def forward(self, x):\n","      # 텐서 대응\n","      self.original_x_shape = x.shape\n","      x = x.reshape(x.shape[0], -1)\n","      self.x = x\n","\n","      out = np.dot(self.x, self.W) + self.b\n","\n","      return out\n","\n","  def backward(self, dout):\n","      dx = np.dot(dout, self.W.T)\n","      self.dW = np.dot(self.x.T, dout)\n","      self.db = np.sum(dout, axis=0)\n","\n","      dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n","      return dx\n","\n","class SoftmaxWithLoss:\n","  def __init__(self):\n","      self.loss = None # 손실함수\n","      self.y = None    # softmax의 출력\n","      self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n","\n","  def forward(self, x, t):\n","      self.t = t\n","      self.y = softmax(x)\n","      self.loss = cross_entropy_error(self.y, self.t)\n","\n","      return self.loss\n","\n","  def backward(self, dout=1):\n","      batch_size = self.t.shape[0]\n","      if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n","          dx = (self.y - self.t) / batch_size\n","      else:\n","          dx = self.y.copy()\n","          dx[np.arange(batch_size), self.t] -= 1\n","          dx = dx / batch_size\n","\n","      return dx\n","\n","\n","def numerical_gradient(f, x):\n","    h = 1e-4 # 0.0001\n","    grad = np.zeros_like(x)\n","\n","    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n","    while not it.finished:\n","        idx = it.multi_index\n","        tmp_val = x[idx]\n","        x[idx] = float(tmp_val) + h\n","        fxh1 = f(x) # f(x+h)\n","\n","        x[idx] = tmp_val - h\n","        fxh2 = f(x) # f(x-h)\n","        grad[idx] = (fxh1 - fxh2) / (2*h)\n","\n","        x[idx] = tmp_val # 값 복원\n","        it.iternext()\n","\n","    return grad\n"],"metadata":{"id":"QNZCKSdHzI1N","executionInfo":{"status":"ok","timestamp":1727194618316,"user_tz":-540,"elapsed":400,"user":{"displayName":"최승훈","userId":"08625075445342019551"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["class SimpleConvNet:\n","    \"\"\"단순한 합성곱 신경망\n","\n","    conv - relu - pool - affine - relu - affine - softmax\n","\n","    Parameters\n","    ----------\n","    input_size : 입력 크기（MNIST의 경우엔 784）\n","    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n","    output_size : 출력 크기（MNIST의 경우엔 10）\n","    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n","    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n","        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n","        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n","    \"\"\"\n","    def __init__(self, input_dim=(1, 28, 28),\n","                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n","                 hidden_size=100, output_size=10, weight_init_std=0.01):\n","        filter_num = conv_param['filter_num']\n","        filter_size = conv_param['filter_size']\n","        filter_pad = conv_param['pad']\n","        filter_stride = conv_param['stride']\n","        input_size = input_dim[1]\n","        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n","        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n","\n","        # 가중치 초기화\n","        self.params = {}\n","        self.params['W1'] = weight_init_std * \\\n","                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n","        self.params['b1'] = np.zeros(filter_num)\n","        self.params['W2'] = weight_init_std * \\\n","                            np.random.randn(pool_output_size, hidden_size)\n","        self.params['b2'] = np.zeros(hidden_size)\n","        self.params['W3'] = weight_init_std * \\\n","                            np.random.randn(hidden_size, output_size)\n","        self.params['b3'] = np.zeros(output_size)\n","\n","        # 계층 생성\n","        self.layers = OrderedDict()\n","        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n","                                           conv_param['stride'], conv_param['pad'])\n","        self.layers['Relu1'] = Relu()\n","        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n","        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n","        self.layers['Relu2'] = Relu()\n","        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n","\n","        self.last_layer = SoftmaxWithLoss()\n","\n","    def predict(self, x):\n","        for layer in self.layers.values():\n","            x = layer.forward(x)\n","\n","        return x\n","\n","    def loss(self, x, t):\n","        \"\"\"손실 함수를 구한다.\n","\n","        Parameters\n","        ----------\n","        x : 입력 데이터\n","        t : 정답 레이블\n","        \"\"\"\n","        y = self.predict(x)\n","        return self.last_layer.forward(y, t)\n","\n","    def accuracy(self, x, t, batch_size=100):\n","        if t.ndim != 1 : t = np.argmax(t, axis=1)\n","\n","        acc = 0.0\n","\n","        for i in range(int(x.shape[0] / batch_size)):\n","            tx = x[i*batch_size:(i+1)*batch_size]\n","            tt = t[i*batch_size:(i+1)*batch_size]\n","            y = self.predict(tx)\n","            y = np.argmax(y, axis=1)\n","            acc += np.sum(y == tt)\n","\n","        return acc / x.shape[0]\n","\n","    def numerical_gradient(self, x, t):\n","        \"\"\"기울기를 구한다（수치미분）.\n","\n","        Parameters\n","        ----------\n","        x : 입력 데이터\n","        t : 정답 레이블\n","\n","        Returns\n","        -------\n","        각 층의 기울기를 담은 사전(dictionary) 변수\n","            grads['W1']、grads['W2']、... 각 층의 가중치\n","            grads['b1']、grads['b2']、... 각 층의 편향\n","        \"\"\"\n","        loss_w = lambda w: self.loss(x, t)\n","\n","        grads = {}\n","        for idx in (1, 2, 3):\n","            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n","            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n","\n","        return grads\n","\n","    def gradient(self, x, t):\n","        \"\"\"기울기를 구한다(오차역전파법).\n","\n","        Parameters\n","        ----------\n","        x : 입력 데이터\n","        t : 정답 레이블\n","\n","        Returns\n","        -------\n","        각 층의 기울기를 담은 사전(dictionary) 변수\n","            grads['W1']、grads['W2']、... 각 층의 가중치\n","            grads['b1']、grads['b2']、... 각 층의 편향\n","        \"\"\"\n","        # forward\n","        self.loss(x, t)\n","\n","        # backward\n","        dout = 1\n","        dout = self.last_layer.backward(dout)\n","\n","        layers = list(self.layers.values())\n","        layers.reverse()\n","        for layer in layers:\n","            dout = layer.backward(dout)\n","\n","        # 결과 저장\n","        grads = {}\n","        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n","        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n","        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n","\n","        return grads\n","\n","    def save_params(self, file_name=\"params.pkl\"):\n","        params = {}\n","        for key, val in self.params.items():\n","            params[key] = val\n","        with open(file_name, 'wb') as f:\n","            pickle.dump(params, f)\n","\n","    def load_params(self, file_name=\"params.pkl\"):\n","        with open(file_name, 'rb') as f:\n","            params = pickle.load(f)\n","        for key, val in params.items():\n","            self.params[key] = val\n","\n","        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n","            self.layers[key].W = self.params['W' + str(i+1)]\n","            self.layers[key].b = self.params['b' + str(i+1)]"],"metadata":{"id":"h2X6c7hdpf3R","executionInfo":{"status":"ok","timestamp":1727194692068,"user_tz":-540,"elapsed":405,"user":{"displayName":"최승훈","userId":"08625075445342019551"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["class Adam:\n","\n","    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\n","\n","    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n","        self.lr = lr\n","        self.beta1 = beta1\n","        self.beta2 = beta2\n","        self.iter = 0\n","        self.m = None\n","        self.v = None\n","\n","    def update(self, params, grads):\n","        if self.m is None:\n","            self.m, self.v = {}, {}\n","            for key, val in params.items():\n","                self.m[key] = np.zeros_like(val)\n","                self.v[key] = np.zeros_like(val)\n","\n","        self.iter += 1\n","        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n","\n","        for key in params.keys():\n","            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n","            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n","            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n","            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n","\n","            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n","\n","            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n","            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n","            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)\n","\n","class Trainer:\n","    \"\"\"신경망 훈련을 대신 해주는 클래스\n","    \"\"\"\n","    def __init__(self, network, x_train, t_train, x_test, t_test,\n","                 epochs=20, mini_batch_size=100,\n","                 optimizer='SGD', optimizer_param={'lr':0.01},\n","                 evaluate_sample_num_per_epoch=None, verbose=True):\n","        self.network = network\n","        self.verbose = verbose\n","        self.x_train = x_train\n","        self.t_train = t_train\n","        self.x_test = x_test\n","        self.t_test = t_test\n","        self.epochs = epochs\n","        self.batch_size = mini_batch_size\n","        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\n","\n","        # optimzer\n","        optimizer_class_dict = {'adam':Adam}\n","        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n","\n","        self.train_size = x_train.shape[0]\n","        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)\n","        self.max_iter = int(epochs * self.iter_per_epoch)\n","        self.current_iter = 0\n","        self.current_epoch = 0\n","\n","        self.train_loss_list = []\n","        self.train_acc_list = []\n","        self.test_acc_list = []\n","\n","    def train_step(self):\n","        batch_mask = np.random.choice(self.train_size, self.batch_size)\n","        x_batch = self.x_train[batch_mask]\n","        t_batch = self.t_train[batch_mask]\n","\n","        grads = self.network.gradient(x_batch, t_batch)\n","        self.optimizer.update(self.network.params, grads)\n","\n","        loss = self.network.loss(x_batch, t_batch)\n","        self.train_loss_list.append(loss)\n","        if self.verbose: print(\"train loss:\" + str(loss))\n","\n","        if self.current_iter % self.iter_per_epoch == 0:\n","            self.current_epoch += 1\n","\n","            x_train_sample, t_train_sample = self.x_train, self.t_train\n","            x_test_sample, t_test_sample = self.x_test, self.t_test\n","            if not self.evaluate_sample_num_per_epoch is None:\n","                t = self.evaluate_sample_num_per_epoch\n","                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\n","                x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]\n","\n","            train_acc = self.network.accuracy(x_train_sample, t_train_sample)\n","            test_acc = self.network.accuracy(x_test_sample, t_test_sample)\n","            self.train_acc_list.append(train_acc)\n","            self.test_acc_list.append(test_acc)\n","\n","            if self.verbose: print(\"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc) + \" ===\")\n","        self.current_iter += 1\n","\n","    def train(self):\n","        for i in range(self.max_iter):\n","            self.train_step()\n","\n","        test_acc = self.network.accuracy(self.x_test, self.t_test)\n","\n","        if self.verbose:\n","            print(\"=============== Final Test Accuracy ===============\")\n","            print(\"test acc:\" + str(test_acc))"],"metadata":{"id":"Hr4C_qqC4LJt","executionInfo":{"status":"ok","timestamp":1727195695201,"user_tz":-540,"elapsed":374,"user":{"displayName":"최승훈","userId":"08625075445342019551"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.datasets import mnist\n","(x_train, t_train), (x_test, t_test) = mnist.load_data()\n","\n","x_train = np.expand_dims(x_train, axis=1)\n","x_test = np.expand_dims(x_test, axis=1)\n","\n","\n","print(x_train.shape)\n","print(t_train.shape)\n","print(x_test.shape)\n","print(t_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n8xf3yZ02b9-","executionInfo":{"status":"ok","timestamp":1727195369233,"user_tz":-540,"elapsed":404,"user":{"displayName":"최승훈","userId":"08625075445342019551"}},"outputId":"448482c7-4449-44d6-86e6-17aa8e0a4573"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 1, 28, 28)\n","(60000,)\n","(10000, 1, 28, 28)\n","(10000,)\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# 시간이 오래 걸릴 경우 데이터를 줄인다.\n","x_train, t_train = x_train[:5000], t_train[:5000]\n","x_test, t_test = x_test[:1000], t_test[:1000]\n","\n","max_epochs = 20\n","\n","cnn = SimpleConvNet(input_dim=(1,28,28),\n","                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n","                        hidden_size=100, output_size=10, weight_init_std=0.01)\n","\n","trainer = Trainer(cnn, x_train, t_train, x_test, t_test,\n","                  epochs=max_epochs, mini_batch_size=100,\n","                  optimizer='Adam', optimizer_param={'lr': 0.001},\n","                  evaluate_sample_num_per_epoch=1000)\n","trainer.train()\n","\n","# 매개변수 보존\n","# cnn.save_params(\"params.pkl\")\n","# print(\"Saved Network Parameters!\")\n","\n","# 그래프 그리기\n","markers = {'train': 'o', 'test': 's'}\n","x = np.arange(max_epochs)\n","plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n","plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"accuracy\")\n","plt.ylim(0, 1.0)\n","plt.legend(loc='lower right')\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"REntvGUb3maF","executionInfo":{"status":"error","timestamp":1727196472301,"user_tz":-540,"elapsed":506100,"user":{"displayName":"최승훈","userId":"08625075445342019551"}},"outputId":"d4e26351-de28-4095-dd68-f2f0d5f92fbc"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.08294623225337093\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.04778055314581458\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.06642595419201296\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.04811889470022745\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.053347891571154805\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.12243303014339882\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.051361390531105215\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0958983039165398\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.1185793426040622\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.10992942439805414\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.12709767943032882\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.09552963905565819\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.08843860237620321\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.08146794777681281\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.06183129050283342\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.018234495612024178\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.03555121987655768\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.09484911202776025\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.11885894269075897\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.06320575107120033\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.03936727840097125\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.017288086136434754\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.047167324633694364\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","=== epoch:3, train acc:0.979, test acc:0.953 ===\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.03630747878558862\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.07034718442542892\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.048883921188338376\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.08448597980838299\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.04430387936449898\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0863889152480745\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.06108941279392037\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.04414650877652405\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.09326165053654215\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.012449189888843547\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.014571869339854285\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.03613457468604239\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.03221784251693454\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.03141828048435405\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.013560096396793096\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.07209604964539816\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.09900949545733678\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.03492199022037881\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.09028481969561307\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.019893811013706537\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.04760003608140202\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.042762924004526894\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.06699757547115004\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.03063473995430572\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.033139245610421676\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0727123102882186\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.035883403776536316\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.02019126160782053\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0386322955397151\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.06924533054252684\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.01329843201931189\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.05006150720135236\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.05895740361269076\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.02021723408098276\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.031214214716489906\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.040193649771237384\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.03211496003518875\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.03127175491577491\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.021787380358747065\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.012996717036799877\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0176842794958901\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.05180874777441741\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0657521568359666\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.01660361787667561\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.02330800176094986\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.008216647682181001\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.012244784357255534\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0614589725654207\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.032089607550356614\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.01830040838529153\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","=== epoch:4, train acc:0.984, test acc:0.962 ===\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.016770533817337042\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0747033357094899\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.09471404023001756\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.01247117161255801\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.025416454547500918\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.012672830908562267\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.017353669832644178\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.09516392537000326\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.026870699435994343\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.03974910365631752\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.012814367782839177\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.024429383293601056\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.01198942421299943\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.015370954092426796\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.023532428640180392\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.06700963815062465\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.01736970507601305\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.03478207086414795\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.009533311846877076\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.05044964407291872\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.009304593918574853\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.02017248174667913\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.03348372371081974\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.025933664234341985\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.01966791692822359\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.10344846301471096\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.012839032759437308\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.02341066430556178\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00971620113636722\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.010010596193810412\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.030249458409184523\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.014077790313519651\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.005661867385318436\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0275336389709424\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.011032964511113465\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.02842352222569485\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.01805506858043411\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.011299173870054795\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.01874029545465812\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.012524161592788883\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.02047774644523475\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.02601630851571895\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0036986390415187933\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.03672185082598993\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.005232376403498844\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.010893798354230181\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.012890666088590321\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.007796512103319838\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.006036115860933527\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.05960970630903829\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","=== epoch:5, train acc:0.987, test acc:0.962 ===\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.018793005083415814\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0077739568176926534\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.022799599776381806\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.028188462202048816\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.011458586126472261\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.005675569524342534\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.009752498800296567\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0361013765320308\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.005049132751969142\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.013607016665778371\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00921331986134704\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.003163216030214859\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.03605105851527697\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.012770136757301013\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.019103727091578565\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.008024198980959948\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.014599747110435076\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.006576524392125165\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.007164949234694137\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.012201995242472126\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0533682746787229\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.011431778569039883\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.009542420760233383\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.006527746802816547\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.007963534123540787\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.009976214780237047\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.02060154971890166\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.014317159573930467\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.004485864526982429\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.009473225376374106\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0068479704056662915\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.032113815858747514\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0074599612876982534\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.004576801579113925\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.004825589217365146\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.004426791815252163\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.005205914197177079\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.017080046295090873\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.007247788435055814\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.016512749110241694\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.006725032450439812\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0027882434444277708\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.007212667626044439\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0020681927168195295\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.012266934585805881\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.033383873921905435\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.011779860580836774\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0066560339484092335\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0064399477196760155\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.004816548170277485\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","=== epoch:6, train acc:0.982, test acc:0.956 ===\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.012281149077229693\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.007513370269358721\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.04925968375563723\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.009676341363634383\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.007641649446938406\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.010957071185512497\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.03901994023509765\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.006516687583242966\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.007727939195420531\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.011635454482191032\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.007591331812065716\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.012394628467059476\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.015289452010001594\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.006369527933202173\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.009554324445063506\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.008341826677528337\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.019107134404623253\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.005489355942175318\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0055677996325160664\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.004425950094640627\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.009778653479737838\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0057344789994366015\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.005576146698236063\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.023591963800087906\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.01174681517830855\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.007615114280293333\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.009756946318094772\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.004857801260677464\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.013157849737109577\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.004111124021587965\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.013691803031973322\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.007001470245823705\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0013540926668083402\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.005917556724594771\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.005613208825698874\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0009309677688698331\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.009861191617307016\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00812694798799382\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.002430194430388546\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.008977774296043496\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0009407366150910432\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.006224684618853714\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.005794192555074834\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.006524818410263498\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0031929156749019557\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.007979792268470438\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0074639497123018626\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0038589547266749346\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0036619829860545365\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.004277369199497887\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","=== epoch:7, train acc:0.99, test acc:0.966 ===\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.001944832910799852\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.006407363382089329\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.007945908379625414\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.02859819203991828\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.007544350883030142\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.005158195971605265\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0014403507456175635\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0031735452763250196\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.009556121989985518\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0059889596913887755\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0035686513098282854\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0038898524892302705\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.008928697932471768\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.003211194187456945\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0458467649570625\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0023976557158361163\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.003538943874213798\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.003753956350916408\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.013760831664368875\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.050122167406474905\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.007713218138752974\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0022349913512459845\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00294121920579693\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.011189747414686311\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.025917616089738322\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.004489316521531826\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.011394283910981464\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.011515440702705595\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.016294986961188914\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.008484656271334283\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.008145647146351133\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.007484066345808462\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0052076192637472585\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.013892525440743661\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.006726961456437321\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0047009735445169305\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.004558256182381144\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0027222609047767555\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.002203762195974292\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.004925821181566349\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00535532152109317\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0035321130729497924\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.002026870222870282\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.004084508135166771\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.003321467966879097\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0015217963041561677\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.008107804111495358\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.007039136489375809\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0035059263873181085\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0026317098889022507\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","=== epoch:8, train acc:0.997, test acc:0.963 ===\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0022507930937514602\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.005214003774552373\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0032571055893823426\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.004802306356050672\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0027318229461711896\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00844716955722291\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0014775781352529863\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0026960747281087757\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.002940176692716753\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0018284891011687142\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0023264241101456687\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.004134899645309721\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00490996900421184\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0014368095703641542\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.005593892898972166\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0026850693195200294\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.002707318674410375\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0033751893242777524\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00176325786037302\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.002187100275524488\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0014786815394228611\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.002340981426089734\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0017007880190291487\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0015298675212695601\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.02106468694932443\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.002431112393345958\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0010211139197933592\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.003992876509792589\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0011260454847920604\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0009061157291715219\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0009119116155351293\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.003045394665752629\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.001257273318994965\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0020569865024906396\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00018597136999788658\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0031855019554572544\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00404124720804165\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.001449333859279451\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00031313496631559754\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0029773286876939704\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.004748537881402289\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0017915013452164345\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0034426957279136885\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0045168642091069165\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0027106889334239507\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0014444367252193688\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.002445831979576549\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.010528606767016143\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0009473159639198706\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.000941407411526633\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","=== epoch:9, train acc:0.998, test acc:0.961 ===\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0002721089115003985\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.002984584421815064\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.002281228381565842\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.002926197196621304\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.014610013629690137\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.004389175696322145\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.002001409518526049\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0070006112233962844\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.006777309944720904\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0027725814897349445\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.000711821272329908\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.012179911711375694\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0030351173293603267\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0023839491563783714\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0033868664751837583\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.01012743108935598\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.002331519852653386\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0005641498689789688\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0007571742439358674\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0025415940815463737\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.002613130707908323\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0009895697475506557\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0009186602643445039\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00161772777478811\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0008971914970214031\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0015639739151679347\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.007387251509744163\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.002037852376190224\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.002581472863377143\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00082254316574847\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.006480170829846948\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00045767402221455326\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.001826144718297166\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0024090859961427543\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0011466690678898072\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0026295244608649944\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0046256031918148295\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00038368444966301886\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.002039640243032141\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0009250187750810619\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.002287366633756815\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.005237157960348781\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0019782612132879566\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0013383487233152613\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0015486793234110049\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.004694916246927212\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0015077175644835322\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0006368205055065655\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.001830534676940533\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0029095029200844136\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","=== epoch:10, train acc:0.999, test acc:0.966 ===\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0017930389749450592\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.001712944894792254\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0016945798225215594\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0014589198173357624\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.000318144675589606\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0008228215581835121\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0012454731619921762\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0002432191008926763\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.001063489363745164\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0014653415976849115\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0020858772544790744\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0009680462818004275\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.000773925167807691\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0015052547518736362\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0004853737206049993\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0010311019564978405\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00021688009766107546\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0007649711119355611\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.001501709126705753\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0011557906724814129\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.001564788640911039\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0034257657375180617\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00112439880734261\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0035238966110738507\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00231984627016439\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.002546572202067693\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00141604488324316\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0016079445945565945\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00474295945478037\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0016519476401996316\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0013997050860374155\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0012785360052442922\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.002175966212777706\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.000818079373154302\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.002677582757584424\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0009732377722719254\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0008040281507783729\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.001833690861498694\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0004995428144889929\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.000833422167845004\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0008601190568083755\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0024360266367175316\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00036712142506835994\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0022593068013085933\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0031822212227172087\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0034331867864505487\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.001493734318263057\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00036827599121297774\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0007404543088748905\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00169657294132243\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","=== epoch:11, train acc:0.998, test acc:0.962 ===\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0027657041017945727\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0006020328860493588\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0011224767905372963\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.003947147136501892\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0007224269095796876\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0021151816166668897\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.003941378092083129\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0008867982881777867\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0011727910632799094\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0042956409913873644\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0015985092474632132\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0031021382377873935\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0039061995560897684\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0018008408164153131\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0025074939871678253\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0005582246707287792\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0009696677014566016\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.004291318407517535\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.009920018201316385\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.005652044477061111\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.005229745396704265\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00249244269918528\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.001411252177853461\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0019242391725508377\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.03335950282518256\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.004702637515826209\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.02137101453177572\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.029508361661525977\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0008927632050097467\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0017213109652807212\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.007741907934549759\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0017455288579999646\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.08907508996880505\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.008096457695681356\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0036356428298274478\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.013920656807325132\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.007070375362938482\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.007255820186042669\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0057059867247514886\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0069892511347614235\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.005110715241750466\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0031112192433550164\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0071720855940204595\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00386259461989988\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.010039534981961641\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.004163592422015302\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0057243406496476825\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0025167575361242896\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0027275047758522544\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.004247058020136751\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","=== epoch:12, train acc:0.996, test acc:0.961 ===\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.002066371832256936\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0023279535239199307\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.028057562654612837\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0042489891382846414\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0016687185735392185\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.008127366368978328\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.010350510995280076\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.005082609304920509\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0036818024357548355\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.004204061666214503\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0016189581846095017\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.006433686576418023\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0013780627861225373\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0047524897989610435\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.002674201656325644\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0031028288164331773\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.003990867328585371\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0035014735503457266\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.007826569018840436\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.005822734763177263\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0028885021062528366\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.003885767106037969\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.004366286704522635\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0018482398240922036\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0042694210777702545\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0007726683600620255\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.004290453672192479\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0018877758228154121\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.001391614927286902\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0058288293219652424\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.005857994133418427\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0005956911755814509\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.001084472740771509\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0013187330951199812\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0034862171580829935\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0030316957631155157\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.002537777416726621\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00272982530338554\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0016527476208752165\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.003942742420002161\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0012956095995307426\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0040400801877680595\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0009315300699669431\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0010264012627100255\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.002891636745422447\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0014827914646670384\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0007970773429998761\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.002497427322793434\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0010020312059254761\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0006812968791940782\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","=== epoch:13, train acc:0.995, test acc:0.96 ===\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00034585660617437184\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0028156131176153548\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.002431709349915682\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0021854170469347524\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.002023530764591386\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0020742330850885933\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0017474694319056776\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.001600124488106814\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.011055136940215602\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0006301847925546541\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.002150293012648766\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0021535873685711963\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0004658641799780058\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0015600894287115241\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0033744734221370807\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00392422867772843\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0006627126272503784\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0024354166037390003\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0019849402392573463\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0008962617714889277\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0007347700212085379\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0015167236357862255\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0007703542563682981\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00010744757790378016\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0024382497712493647\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.001765263455711213\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.005276621719557355\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0015148549792647974\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.001992453208904373\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0011855675372248419\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0006460795940322721\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0006442829782663683\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0026906102003477956\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0007340895826806673\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0002747995156413285\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0012724777538576715\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0005137548613863255\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.002079854720542548\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0004216089972439673\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0003361310020936783\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00033388633317117536\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00022973744701369758\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00045078415751414054\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.001727606320020812\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0002721325087266072\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00026021020655310413\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0008367490373180581\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0012462838347135972\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0007806748219613775\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00022614710153118694\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","=== epoch:14, train acc:0.999, test acc:0.972 ===\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.000175531630083984\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0001946464645636039\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00029242593407064935\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0010936817442339466\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0004861510082385451\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0005563629987069302\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0008552299149289834\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0008684746324909594\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00012180873661131358\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00031004568978089256\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0007434052367057313\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:5.109519776791861e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0010428097650671867\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.000771421200855258\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0001007875508529979\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0003132750346674416\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0018657702475026435\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:7.559008183597891e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0010813836014627158\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0006799011373081934\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:4.018700255767623e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0006014248886786174\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0003860754671996741\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00044368887806325624\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0006974405443708824\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00019811087872336305\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.000408563532258134\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0004319225443577166\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0002764162489643488\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.001930643850982251\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.000370770921046285\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0014167833516626493\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00013154188682747205\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0023486193286532528\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00022050863985204001\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0002881802451990764\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00016291346172223956\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00015235989396936088\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:6.508972715054281e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.000575860338600443\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00010892290802447793\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00036550212564399984\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00046917163956946553\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:9.289927472437838e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00011981663513501052\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0003015953344954122\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0009235698397938126\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00030815758310722194\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00048318819723587646\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:7.306079119450609e-05\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","=== epoch:15, train acc:0.999, test acc:0.97 ===\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0012144429910679226\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00019059868264480483\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0001278034168548743\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0003282783276312648\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0005076466094711813\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0006411770374425636\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00040700851802194244\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00029271868334495026\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00038385756737951304\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0005703534647987458\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0006970344128148362\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0003093366526842468\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00017475991644426458\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0011196596359407647\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:5.8261667699711266e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00022876872729119105\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.000394584990483363\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0003669794418722223\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00016869698539325188\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.000131094666328029\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00016809682789773633\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0001493294277629718\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.001066108747809458\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.001301552737881386\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.000361175724126342\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00018417652248641615\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0001424638207528744\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00028701038633223625\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:6.798605160194991e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0002812182903393779\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:7.74354510455302e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:3.30029801056357e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00013570294461764717\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:8.893027823569688e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.000114785338524448\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0002496426046339648\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00012602332396796705\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0010271953473641796\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00016047972660003084\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0001510250463853082\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00014332994960194827\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00039258086067378865\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00034724417898277475\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:5.8050250153807026e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00014346560012158452\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0003797075187151259\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0001932315314483313\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00023884106545259427\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:3.648943422177664e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00028617213049069996\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","=== epoch:16, train acc:0.999, test acc:0.97 ===\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:6.539126208448173e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00013976027329679703\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:9.210275048390205e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:9.777484241242171e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00010892274865734483\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0004823271852690089\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0005523854690732719\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.000715194116176257\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0055396042264429755\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0013890523998094343\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.02499475472009898\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:9.26121951227134e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00010923354763328704\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0001282519599285211\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0003744897280412836\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0018787058902150152\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0017118628551975957\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00016172891870727606\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0013862483574645411\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0004554949908833851\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.04114422933302194\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:6.988933241181656e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0007546274241224241\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00031924142080295016\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0007423069538312041\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0002845963942416553\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0012843112522495132\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0001049276152300111\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0004030233335673172\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0001716859105616043\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0002672522535274115\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0009806609600982689\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0009956880650560547\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.000715812081027553\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0006663669115155773\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0006116763801614814\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0002359331870130434\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00011958910525382914\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0003846476088948618\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0004261240157491272\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00020156120307812545\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00017599406802837087\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00031235480506573216\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0008914313241371364\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0008547308663602183\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0001834453336230813\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00014306234781268846\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0006320138578661989\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00021900442370322433\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.001025357244011161\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","=== epoch:17, train acc:1.0, test acc:0.967 ===\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00012917854552491268\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0002380604825197953\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00025798966055550024\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00015884075835039614\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00010801684950269242\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.000737869851025572\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0012269624098908214\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00022392177847136514\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00039402739574911303\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00037263024855731025\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.000691865476967437\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0004625122022221534\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0005638583696035666\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0005987751567001502\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0001320863363540491\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00029201524836548537\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:4.808713294522857e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:5.6120633507173665e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00047102974136710186\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00022915562427720272\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.000498089151093247\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0004791109769287587\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0002585795494184368\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0002928795237722083\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.000646988609325323\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00043786558060810376\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0002386459835816119\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00013006678223112604\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.001078771580729797\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0001774549108278908\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00034297193167079235\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00032871557052464173\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.001385392380266125\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00015317485979587215\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00039139919113323387\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00020818850203149704\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00010180482026814805\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:7.540667309774073e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00010840530702650829\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00048303063937392405\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00018948515432132295\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00017440688103965033\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00013042570381869027\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0001237455693676881\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0008831243162926846\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0002444802256913288\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0006646285509162886\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0009811597204306519\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.001057852162404712\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0002643738621912294\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","=== epoch:18, train acc:0.998, test acc:0.962 ===\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0004960702096180682\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0008869567315548022\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0016193962143154713\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00045227286242946544\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0003642083450133773\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:3.245201083907897e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0005806611462374074\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.000557167373747715\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0004734245518697451\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00023649353751952276\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0005982684634157916\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0005718692025318254\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0003319431227723005\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.000461362542135457\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0005330550022292295\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0003116159944382047\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00015531262709537208\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00037127361886606\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.000994843956674983\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0006337004654319934\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00040023592739479906\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0006406227251528636\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0007980270150610851\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00033105996714314614\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0009256682318538699\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00010057624708019728\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0441940022992381\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00043441560619975674\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0003876155059052897\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00016466295969616609\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:1.9792381895902905e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0006928890972808617\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0007316660933469139\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.005284355166845734\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0017781065608597846\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:8.947381065018899e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0014696728997822819\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.004494385127203085\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0012044944613836897\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00013215303507286232\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0005435837107833148\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00041519663286920013\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0003363751915364377\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0006139147885141666\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00026246545462284065\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0003375216516428903\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0010286429115741094\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0016870043414088979\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.000409050343840539\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0013949661650422807\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","=== epoch:19, train acc:0.998, test acc:0.969 ===\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0013780781448557544\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00019763514232020379\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.001338086531787909\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0004899947503561828\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0005658629455863346\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0016017388092657014\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00119255711774381\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:9.895371959337635e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00038836219538662404\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0005667714275456287\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:4.8027623038073504e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00014040286261521803\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0005050962562690505\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:4.380119065346876e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0005422844485944101\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00010566912258183499\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00012559444238337047\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:5.957290239868274e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00014020682051586652\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.000666742320601236\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0010432756737301064\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00011582873568724947\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00020208262983399523\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0002445258347239861\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0003159459397373273\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0001873424749263979\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:9.950304910944969e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00021457498088316775\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0005218525841145269\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0002187284700757311\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00028232017831742655\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0004025010135788712\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0002577547262799646\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0002394876420236013\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0006403113306790824\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0002453905611810666\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00012311512969235346\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0002676491062386676\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.000228995328922254\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0004476352642895633\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0001281456324721461\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00014820136990911195\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0003739045151173479\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:9.946403487724001e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0003184110114055812\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00023178046139790836\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00024455581164855515\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0001772544748363849\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00012730830843567012\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00018302090295391565\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","=== epoch:20, train acc:1.0, test acc:0.973 ===\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00017214607455984872\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:6.588868404265245e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00017636267584561241\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00010575288486648125\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00014454710934449992\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:3.760600223865388e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:5.952214470207215e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00029205873268308807\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00021440685058530019\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00016274926268670053\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0007852947670767719\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:4.7633602451007656e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:7.802506243615673e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:6.615576698223172e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00015006021187113222\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0002342594667167619\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00011997583931910978\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0003931749304557713\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00029296848131220494\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00015571853606447138\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00023169023985737195\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0005949282600432854\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0025262062474316733\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00018432992832925277\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0002391046469574695\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:2.6607884562348375e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00021955395977990309\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00014311802105116002\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00022045943868780407\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00011093853806973055\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00012813518871456\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.000309538527292261\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:5.2436303807423206e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:4.1506333688253494e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:3.880878008006248e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:9.71275761958232e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:4.420641943939159e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00016997057259939084\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:2.5676936752721903e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.00012512286619978562\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:6.820624041557558e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:9.022430071420917e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:0.0005945234325062452\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:9.274608149553685e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:3.623446353310821e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:4.5472326514808586e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:5.3829883214648094e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:5.790909544780063e-05\n","12 12\n","(100, 12, 12, 30)\n","(100, 12, 12, 30, 4)\n","12 12\n","train loss:1.1942206397081755e-05\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","12 12\n","=============== Final Test Accuracy ===============\n","test acc:0.972\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'pickle' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-44-97da6f51c3db>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# 매개변수 보존\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"params.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saved Network Parameters!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-31-fda5ce0bd0e6>\u001b[0m in \u001b[0;36msave_params\u001b[0;34m(self, file_name)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"params.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"]}]},{"cell_type":"markdown","source":["## 7.6 CNN 시각화하기\n","* 필터가 보고 있는 것\n","  1. edge : 색상이 바뀐 경계선\n","  2. blob : 국소적으로 덩어리진 영역\n","* 딥러닝 시각화에 관한 연구에 따르면 conv&pool 계층을 깊게 쌓을수록 추출되는 정보가 더 추상화 된다"],"metadata":{"id":"saob4hRxZ96f"}},{"cell_type":"markdown","source":["## 7.7 대표적인 CNN\n","LeNet\n","* sigmoid를 사용하는 데 반해, 현재는 주로 ReLU 사용\n","* subsampling하는 pooling을 사용하는 데 반해, 현재는 max pooling이 주류    \n","\n","AlexNet\n","* ReLU 사용\n","* Local Response Normalization이라는 국소적 정규화를 실시하는 계층 존재\n","* dropout 사용\n"],"metadata":{"id":"L8nYYZl3YwVp"}},{"cell_type":"code","source":[],"metadata":{"id":"oBwH2japZ0im"},"execution_count":null,"outputs":[]}]}